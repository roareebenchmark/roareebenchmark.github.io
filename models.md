---
layout: default
title: LLM Models
nav_order: 2
---

# Evaluated LLM Models

Roaree Benchmark rigorously tests industry-leading models to determine their performance, accuracy, and reliability in academic and professional contexts.

### Top Evaluated LLMs
We currently prioritize benchmarking the following high-impact models:

1. **GPT-4o (OpenAI):** The industry standard for multimodal reasoning and complex instruction following.
2. **Claude 3.5 Sonnet (Anthropic):** Highly regarded for its nuanced writing style and advanced coding capabilities.
3. **Gemini 1.5 Pro (Google):** Featuring a massive context window for deep document analysis.
4. **Llama 3 (Meta):** The leading open-weights model for local deployment and fine-tuning research.
5. **Mistral Large 2 (Mistral AI):** A high-efficiency European model optimized for multilingual tasks and reasoning.

---

### Reviewing & Grading HuggingFace Models
Beyond the major providers, we actively grade open-source models hosted on **HuggingFace**. 

Our team evaluates these models based on:
* **Inference Efficiency:** Performance vs. computational cost.
* **Truthfulness:** Propensity for hallucinations in specialized datasets.
* **Safety & Bias:** Alignment with academic and ethical standards.



---
*Interested in testing a specific model? Join the project team to suggest a benchmark candidate.*
