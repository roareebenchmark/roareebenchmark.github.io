<img src="./assets/roaree-bench-logo.png" alt="Roaree Bench Logo" width="80" style="display: block; margin: 0 auto;">

# Roaree LLM Model Benchmark Club

[APPLY TO TEAM](https://forms.gle/bTBuDyHQc46jLCDm7) | [HOW TO CONTRIBUTE](./CONTRIBUTING.md)

**Columbia University's LLM Benchmark Initiative**

**Roaree Benchmark** is a student-led research initiative at Columbia University dedicated to the rigorous evaluation of state-of-the-art **Large Language Models (LLMs)**.

### üî¨ Models Under Evaluation
We will be benchmarking the industry's "Top 4" flagship models to establish our baseline:

1. **OpenAI GPT-4o** ‚Äì Testing reasoning and multimodal capabilities.
2. **Anthropic Claude 3.5 Sonnet** ‚Äì Evaluating coding proficiency and nuance.
3. **Google Gemini 1.5 Pro** ‚Äì Assessing long-context window performance.
4. **Meta Llama 3.1 (405B)** ‚Äì Analyzing open-weights vs. closed-source performance.

This is an **invite-only** collaborative space for vetted students from Columbia University and select external contributors working on responsible AI assessment and transparency.

## üéØ Mission

Our mission is to develop rigorous, transparent, and reproducible benchmarks for evaluating LLM capabilities, limitations, and safety characteristics. We aim to advance the field of AI evaluation through collaborative research and open scientific discourse.

## üîí Membership

**This repository is invite-only.** Access is restricted to:

- Vetted Columbia University students with demonstrated interest in AI research
- Select external collaborators approved by the core team
- Faculty advisors and research mentors

### How to Request Access

If you are a Columbia student interested in joining:
1. Review the [Membership Application](https://forms.gle/ugp8SMPUPA3qae9X7)
2. Submit your completed application through official Columbia channels
3. Wait for the vetting process to complete

---

*Roar, Lions, Roar! ü¶Å*
