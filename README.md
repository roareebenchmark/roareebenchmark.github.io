<img src="./assets/roaree-bench-logo.png" alt="Roaree Bench Logo" width="80" style="display: block; margin: 0 auto;">

# Roaree LLM Benchmark Club

[APPLY TO TEAM](https://forms.gle/bTBuDyHQc46jLCDm7) | [BENCHMARKS](./benchmarks.md) | [AI AGENTS](./agents.md)  

**Columbia University's LLM Benchmark Initiative**

**Roaree Benchmark** is a student-led research initiative at Columbia University dedicated to the evaluation and testing of new state-of-the-art **Large Language Models (LLMs)**.

Our goal is to test new LLMs and Agents then release our ratings and collaborative research papers on these models for industry review.

### üî¨ Models Under Evaluation
We will be benchmarking the industry's "Top" LLM models to establish our baseline:

1. **OpenAI GPT** ‚Äì Testing reasoning and multimodal capabilities.
2. **Anthropic Claude** ‚Äì Evaluating coding proficiency and nuance.
3. **Google Gemini** ‚Äì Assessing long-context window performance.
4. **Meta Llama** ‚Äì Analyzing open-weights vs. closed-source performance.

This is an **collaborative space** for  students from Columbia University working on responsible AI assessment and transparency.

**Have a research paper? Contact us**

## üéØ Mission

Our mission is to develop rigorous, transparent, and reproducible benchmarks for evaluating LLM capabilities, limitations, and safety characteristics. We aim to advance the field of AI evaluation through collaborative research and open scientific discourse by leveraging the expertise of Columbia University.

## üîí Student Membership

**This project is opoen to all students**

- Columbia University students with demonstrated interest in AI research
- Select external collaborators approved by the core team
- Faculty advisors and research mentors

### How to Request Access

If you are a Columbia student interested in joining:
1. Fill out [Membership Application](https://forms.gle/ugp8SMPUPA3qae9X7)
2. We'll email you official **Whatsapp** group to join

---

## üçï Student Meetups: Testing & Pizza
**Where LLM research meets Columbia culture.**

LLM testing doesn't have to be dry. We plan to host regular technical meetups (live & virtual) to break down the latest AI models and agents in a collaborative environment.

* **LLM Grading Sessions:** Join the "War Room" to stress-test and grade the newest, most dynamic LLM releases the moment they hit the news.
* **Fuel Your Research:** We keep the brainpower running with **pizza and snacks.**
* **Build Your Network:** Connect with other students, alumni, and industry mentors who are as obsessed with the frontier of AI as you are.
* **Make an Impact:** Turn your casual time into a collaborative research sprint that lands on your resume and in global technical reports.

**Summer Program: Virtual Meetups for Students Returning in Fall 2026**

---

*Roar, Lions, Roar! ü¶Å*
