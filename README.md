<img src="./assets/roaree-bench-logo.png" alt="Roaree Bench Logo" width="80" style="display: block; margin: 0 auto;">

# Roaree LLM Model Benchmark Club

[APPLY TO TEAM](https://forms.gle/bTBuDyHQc46jLCDm7) | [HOW TO CONTRIBUTE](./CONTRIBUTING.md)

**Columbia University's LLM Benchmark Initiative**

**Roaree Benchmark** is a student-led research initiative at Columbia University dedicated to the rigorous evaluation of state-of-the-art **Large Language Models (LLMs)**.

### ğŸ”¬ Models Under Evaluation
We will be benchmarking the industry's "Top 4" flagship models to establish our baseline:

1. **OpenAI GPT-4o** â€“ Testing reasoning and multimodal capabilities.
2. **Anthropic Claude 3.5 Sonnet** â€“ Evaluating coding proficiency and nuance.
3. **Google Gemini 1.5 Pro** â€“ Assessing long-context window performance.
4. **Meta Llama 3.1 (405B)** â€“ Analyzing open-weights vs. closed-source performance.

This is an **invite-only** collaborative space for vetted students from Columbia University and select external contributors working on responsible AI assessment and transparency.

## ğŸ¯ Mission

Our mission is to develop rigorous, transparent, and reproducible benchmarks for evaluating LLM capabilities, limitations, and safety characteristics. We aim to advance the field of AI evaluation through collaborative research and open scientific discourse.

## ğŸ”’ Membership

**This repository is invite-only.** Access is restricted to:

- Vetted Columbia University students with demonstrated interest in AI research
- Select external collaborators approved by the core team
- Faculty advisors and research mentors

### How to Request Access

If you are a Columbia student interested in joining:
1. Review the [Membership Application](https://forms.gle/MZpzXQ2hXNEMD18o7)
2. Submit your completed application through official Columbia channels
3. Wait for the vetting process to complete

**Note:** Unsolicited pull requests from non-members will not be accepted. Please request membership first.

### New Member Onboarding

Once accepted, new members should read the [Welcome Guide](WELCOME.md) for onboarding instructions.

## ğŸ“‹ What We Do

- Develop novel benchmarks for LLM evaluation
- Conduct comparative analyses of state-of-the-art models
- Research bias, fairness, and safety in AI systems
- Collaborate on academic publications and presentations
- Maintain transparency in AI assessment methodologies

## ğŸ¤ Contributing

Members should review our [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:
- Code standards and review processes
- Research protocols
- Collaboration workflows
- Attribution and authorship policies

## ğŸ“œ Code of Conduct

We are committed to providing a welcoming and professional environment. All members must adhere to our [Code of Conduct](CODE_OF_CONDUCT.md).

## ğŸ“ Contact

For membership inquiries or questions, please reach out through:
- Columbia University official channels
- Repository maintainers (listed in CONTRIBUTORS.md)

## ğŸ“„ License

This project is licensed under the MIT License with additional terms for research use. See [LICENSE](LICENSE) for details.

---

*Roar, Lions, Roar! ğŸ¦*
